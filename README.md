## Abstract

This project is an AI-powered search system designed to retrieve and synthesize relevant biomedical research papers from PubMed. The system reranks and summarizes scientific literature to help researchers quickly find and understand key findings across multiple studies. By fine-tuning a BioBERT-based cross-encoder on biomedical question-answer pairs and integrating a BART summarization model, this project aims to deliver concise, relevant answers to complex research queries. The system achieves improved relevance ranking over baseline keyword search while maintaining fast query response times.

## Overview

### The Problem

Biomedical researchers face an overwhelming volume of scientific literature, with millions of papers published annually on PubMed alone. Traditional keyword-based search often returns hundreds of results, requiring researchers to manually read through abstracts to identify relevant studies. This process is time consuming and inefficient, particularly when trying to synthesize findings across multiple papers or answer specific research questions. The challenge is to build a system that not only retrieves relevant papers but also understands semantic relationships between queries and documents, then presents findings in a digestible format.

### Why This Problem Matters

Efficient literature search is critical for advancing scientific research and evidence-based medicine. Researchers need to quickly identify relevant prior work to avoid duplicating studies, build on existing knowledge, and make informed decisions. In clinical settings, healthcare professionals require rapid access to the latest research to inform treatment decisions. A more intelligent search system can accelerate scientific discovery, improve research quality, and ultimately contribute to better health outcomes.

### Proposed Approach

This project employs a three-stage pipeline: 

1. Initial retrieval using PubMed's search API with query preprocessing.

2. Reranking using a fine-tuned BioBERT cross-encoder to score query-document relevance.

3. Extractive summarization using DistilBART to condense findings from top-ranked papers.

### Rationale

Prior work in biomedical information retrieval has shown that domain-specific language models (like BioBERT) outperform general-purpose models due to their training on medical literature. Cross-encoders, which jointly encode query and document pairs, have demonstrated superior performance over bi-encoders for reranking tasks, though at higher computational cost. Our approach differs from existing systems by integrating reranking with multi-document summarization, providing not just a ranked list but synthesized insights. I chose DistilBART for summarization due to its balance of speed and quality, making the system practical for real-time use.

### Key Components

The system consists of three main components: 
1. A PubMed knowledge base interface that retrieves initial candidates.

2. A BioBERT-based reranker fine-tuned on BioASQ question-answer pairs.

3. A DistilBART summarizer that processes each abstract individually before presenting their brief summaries. 
    
### Result Components
Here are the components of the result:
1. List of top papers

Each paper:
- Title
- Relevance score
- Source ID

## Approach

### Methodology

The overall methodology follows a retrieves then rerank system. First, we preprocess queries and removes stopwords to improve search. These candidates from the inital retrieval are then passed to a trained reranker that computes semantic similarity scores between the query and each paper's title and abstract. Finally, the top 5 reranked papers are summarized individually using a summarization model, with results presented to the user in ranked order.

### Algorithm/Model

**Reranker**: I use a BioBERT-based cross-encoder (dmis-lab/biobert-base-cased-v1.1) and fine-tune it on the BioASQ 12B dataset. The model takes [query, document] pairs as input and outputs a relevance score. Training uses binary cross-entropy loss with positive pairs from question-answer matches and hard negatives. Hard negatives are generated by computing TF-IDF similarity between each query and the corpus, selecting high-similarity but non-relevant documents as negatives. This forces the model to learn fine-grained distinctions between relevant and superficially similar documents.

**Summarizer**: I employ DistilBART-CNN-12-6, a distilled version of BART fine-tuned on CNN/DailyMail news articles so it can perform faster, producing concise 2-3 sentence summaries per paper.

### Assumptions and Design Choices

- **Assumption**: PubMed's initial retrieval has relevant papers in the top 20
- **Choice**: BioBERT over general BERT domain-specific pretraining improves biomedical understanding

### Limitations

- **Coverage**: Due to time, the model only searches PubMed and does not include papers in other databases, although extending it to include that functionality is not difficult. 
- **Query understanding**: Effective queries partly depend on the user.

## Experiments

### Dataset

**Training Data**: BioASQ 12B question-answer-passages dataset, containing biomedical questions paired with relevant text snippets from PubMed abstracts.

**Corpus Statistics**: 


### Implementation

**Models**:
- Reranker: BioBERT cross-encoder (dmis-lab/biobert-base-cased-v1.1)
- Summarizer: DistilBART-CNN-12-6 (sshleifer/distilbart-cnn-12-6) (untrained)

**Parameters**:


**Computing Environment**:

### Model Architecture


## Results

### Main Results

### Supplementary Results


## Discussion

### Comparison with Existing Approaches


### Future Directions

1. **Query Reformulation**: Add a query expansion module to automatically add synonyms and related terms to the query.

2. **Larger Training Set**: Train on more datasets with more compute resources to improve reranker.

3. **Comprehensive Database**: Instead of calling PubMed's API, download its full data with a mechansim to update it daily.

4. **Citation Network**: Incorporate citation relationships to boost papers that are highly cited by other relevant papers

5. **Broader Coverage**: Extend beyond PubMed to include other databases (bioRxiv, medRxiv).

## Conclusion


## References

